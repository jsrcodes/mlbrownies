<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Neural Machine Translation using Encoder-Decoder | Machine Learning Idiot</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Neural Machine Translation using Encoder-Decoder" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Important aspects of Sequence-to-Sequence model" />
<meta property="og:description" content="Important aspects of Sequence-to-Sequence model" />
<link rel="canonical" href="https://jayasimhareddy.github.io/mlidiot/deep%20learning/2021/03/10/seq2seq-nmt.html" />
<meta property="og:url" content="https://jayasimhareddy.github.io/mlidiot/deep%20learning/2021/03/10/seq2seq-nmt.html" />
<meta property="og:site_name" content="Machine Learning Idiot" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-10T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://jayasimhareddy.github.io/mlidiot/deep%20learning/2021/03/10/seq2seq-nmt.html","@type":"BlogPosting","headline":"Neural Machine Translation using Encoder-Decoder","dateModified":"2021-03-10T00:00:00-06:00","datePublished":"2021-03-10T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jayasimhareddy.github.io/mlidiot/deep%20learning/2021/03/10/seq2seq-nmt.html"},"description":"Important aspects of Sequence-to-Sequence model","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/mlidiot/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jayasimhareddy.github.io/mlidiot/feed.xml" title="Machine Learning Idiot" /><link rel="shortcut icon" type="image/x-icon" href="/mlidiot/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/mlidiot/">Machine Learning Idiot</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/mlidiot/about/">About Me</a><a class="page-link" href="/mlidiot/search/">Search</a><a class="page-link" href="/mlidiot/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Neural Machine Translation using Encoder-Decoder</h1><p class="page-description">Important aspects of Sequence-to-Sequence model</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-10T00:00:00-06:00" itemprop="datePublished">
        Mar 10, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/mlidiot/categories/#Deep Learning">Deep Learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><ol>
  <li>Vectorize words (or characters if you are working on character sequences) to encode into numerical respresentation. The training data usually has pairs of source and target sequences $&lt;S, T&gt;$ ordered by time step and grouped into batches.</li>
  <li>Feeding input sequences in a reversed order would help improve training as that will make beginning of sentence to be fed last to the encoder, but that would be first the decoder needs to translate. Note that only the source (for example, Engish sentence) sequences are reversed but the target sequences (for example, French) are fed in their natural order.</li>
  <li>Rather than having a simple one-hot encoding representation for the words, it would be a great uplift for the model if we add an embedding layer that projects words into a low-dimensional space that retains semantic properties of the language. For instance, the words “King”, “Queen” would lie closer in the emdedding space.</li>
  <li>The input embeddings are then fed into encoder, which is typically from a class of RNNs.</li>
  <li>The final state outputted from the encoder is fed as input to the decoder net. In addition to the encoder’s output state, target sequences are fed to the decoder. The decoder will output a score for each word in the target vocabulary, which is converted into a porbability by a softmax layer. Since the probabilities are generated for the entire target language vocabulary, the word with the highest probability is considered.</li>
  <li>It might have popped up in your head that how the decoder works during inference time since the targets will not be available. Well, that is a good question to ask. At the time of inference, the decoder is fed with previously predicted word at each time step and with <em>”&lt;SOS&gt;”</em> at the first step. So, at $t_0$ we feed “<em>&lt;SOS&gt;</em>”, at $t_1$ we feed output of $t_0$ as input, and so on..</li>
  <li>So far so good. But, the output and input sequence may not have same length and can vary across the samples. Masking is one solution which involves cropping extra words after a certain length. It leads to incomplete translations and not advisable. The better alternative for handling variable sequence lengths is to sentences into buckets with each bucket holding sentences of similar sizes (eg., a bucket for sentences u to 6 words, another one having sentences from 7 to 15 words). Then apply padding (fill gaps with special tokens such as <em>&lt;pad&gt;</em>) in each bucket so that all sequences in a bucket have same length.</li>
  <li>It is not efficient to output probabilities for all the words in the vocabulary, instead employ a sampling approach. Sample softmax loss can be employed during training.</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">TrainingSampler</code> and <code class="language-plaintext highlighter-rouge">ScheduledEmbeddingTrainingSample</code> are available in Tensorflow Addons.</p>
  </li>
  <li>
    <p>What is a TimeDistributed layer in keras?</p>
  </li>
  <li>Dimesions in a RNN layer
Weights: $W_x$ for inputs and $W_y$ for outputs of the previous time step</li>
</ol>

<table>
  <thead>
    <tr>
      <th>$Input$</th>
      <th>$(batch, num_steps, feature_dimensions$)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$W_{xh}$</td>
      <td>$(feature_dimensions,hidden)$</td>
    </tr>
    <tr>
      <td>$Hidden$</td>
      <td>$(batch, hidden)$</td>
    </tr>
    <tr>
      <td>$W_{hh}$</td>
      <td>$(hidden,hidden)$</td>
    </tr>
    <tr>
      <td>$W_{hq}$</td>
      <td>$(hidden,output)$</td>
    </tr>
    <tr>
      <td>$Output$</td>
      <td>$(batch,output)$</td>
    </tr>
  </tbody>
</table>

<ol>
  <li>
    <p>How can a RNN handle variable length sequences?</p>
  </li>
  <li>
    <p>How many parameters does an RNN need to learn?</p>
  </li>
  <li>
    <p>How can we improve training time?</p>
  </li>
  <li>
    <p>How to improve covergence of an RNN model?</p>
  </li>
</ol>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jayasimhareddy/mlidiot"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/mlidiot/deep%20learning/2021/03/10/seq2seq-nmt.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/mlidiot/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/mlidiot/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/mlidiot/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My journey through ML Engineering</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jayasimhareddy" title="jayasimhareddy"><svg class="svg-icon grey"><use xlink:href="/mlidiot/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
