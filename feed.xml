<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://jsrcodes.github.io/mlidiot/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jsrcodes.github.io/mlidiot/" rel="alternate" type="text/html" /><updated>2021-03-11T11:02:46-06:00</updated><id>https://jsrcodes.github.io/mlidiot/feed.xml</id><title type="html">Machine Learning Idiot</title><subtitle>My journey through ML Engineering</subtitle><entry><title type="html">Things I frequently use in Tensorflow</title><link href="https://jsrcodes.github.io/mlidiot/python/2021/03/10/tensorflow-freq.html" rel="alternate" type="text/html" title="Things I frequently use in Tensorflow" /><published>2021-03-10T00:00:00-06:00</published><updated>2021-03-10T00:00:00-06:00</updated><id>https://jsrcodes.github.io/mlidiot/python/2021/03/10/tensorflow-freq</id><author><name></name></author><category term="Python" /><summary type="html">Tensorflow’s most powerful tools for Deep Learning</summary></entry><entry><title type="html">Hello world! for Regex</title><link href="https://jsrcodes.github.io/mlidiot/python/2021/03/10/common-regex.html" rel="alternate" type="text/html" title="Hello world! for Regex" /><published>2021-03-10T00:00:00-06:00</published><updated>2021-03-10T00:00:00-06:00</updated><id>https://jsrcodes.github.io/mlidiot/python/2021/03/10/common-regex</id><author><name></name></author><category term="Python" /><summary type="html">Commonly used regex for data preprocessing</summary></entry><entry><title type="html">Tokenizing</title><link href="https://jsrcodes.github.io/mlidiot/deep%20learning/2021/03/10/tokenizing.html" rel="alternate" type="text/html" title="Tokenizing" /><published>2021-03-10T00:00:00-06:00</published><updated>2021-03-10T00:00:00-06:00</updated><id>https://jsrcodes.github.io/mlidiot/deep%20learning/2021/03/10/tokenizing</id><author><name></name></author><category term="Deep Learning" /><summary type="html">creating a space between a word and the punctuation following it 1.</summary></entry><entry><title type="html">Neural Machine Translation using Encoder-Decoder</title><link href="https://jsrcodes.github.io/mlidiot/deep%20learning/2021/03/10/seq2seq-nmt.html" rel="alternate" type="text/html" title="Neural Machine Translation using Encoder-Decoder" /><published>2021-03-10T00:00:00-06:00</published><updated>2021-03-10T00:00:00-06:00</updated><id>https://jsrcodes.github.io/mlidiot/deep%20learning/2021/03/10/seq2seq-nmt</id><author><name></name></author><category term="Deep Learning" /><summary type="html">Vectorize words (or characters if you are working on character sequences) to encode into numerical respresentation. The training data usually has pairs of source and target sequences $&amp;lt;S, T&amp;gt;$ ordered by time step and grouped into batches. Feeding input sequences in a reversed order would help improve training as that will make beginning of sentence to be fed last to the encoder, but that would be first the decoder needs to translate. Note that only the source (for example, Engish sentence) sequences are reversed but the target sequences (for example, French) are fed in their natural order. Rather than having a simple one-hot encoding representation for the words, it would be a great uplift for the model if we add an embedding layer that projects words into a low-dimensional space that retains semantic properties of the language. For instance, the words “King”, “Queen” would lie closer in the emdedding space. The input embeddings are then fed into encoder, which is typically from a class of RNNs. The final state outputted from the encoder is fed as input to the decoder net. In addition to the encoder’s output state, target sequences are fed to the decoder. The decoder will output a score for each word in the target vocabulary, which is converted into a porbability by a softmax layer. Since the probabilities are generated for the entire target language vocabulary, the word with the highest probability is considered. It might have popped up in your head that how the decoder works during inference time since the targets will not be available. Well, that is a good question to ask. At the time of inference, the decoder is fed with previously predicted word at each time step and with ”&amp;lt;SOS&amp;gt;” at the first step. So, at $t_0$ we feed “&amp;lt;SOS&amp;gt;”, at $t_1$ we feed output of $t_0$ as input, and so on.. So far so good. But, the output and input sequence may not have same length and can vary across the samples. Masking is one solution which involves cropping extra words after a certain length. It leads to incomplete translations and not advisable. The better alternative for handling variable sequence lengths is to sentences into buckets with each bucket holding sentences of similar sizes (eg., a bucket for sentences u to 6 words, another one having sentences from 7 to 15 words). Then apply padding (fill gaps with special tokens such as &amp;lt;pad&amp;gt;) in each bucket so that all sequences in a bucket have same length. It is not efficient to output probabilities for all the words in the vocabulary, instead employ a sampling approach. Sample softmax loss can be employed during training. TrainingSampler and ScheduledEmbeddingTrainingSample are available in Tensorflow Addons.</summary></entry><entry><title type="html">Decorators in Python</title><link href="https://jsrcodes.github.io/mlidiot/python/2021/03/09/python-decorators-ds.html" rel="alternate" type="text/html" title="Decorators in Python" /><published>2021-03-09T00:00:00-06:00</published><updated>2021-03-09T00:00:00-06:00</updated><id>https://jsrcodes.github.io/mlidiot/python/2021/03/09/python-decorators-ds</id><author><name></name></author><category term="Python" /><summary type="html"></summary></entry></feed>